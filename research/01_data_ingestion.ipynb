{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\ahmed\\\\OneDrive\\\\Documents\\\\GitHub\\\\Arabic-Sentiment-Analysis-using-Arabic-BERT\\\\twitter_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ø§Ø¹ØªØ±Ù Ø§Ù† Ø¨ØªØ³ ÙƒØ§Ù†Ùˆ Ø´ÙˆÙŠ Ø´ÙˆÙŠ ÙŠØ¬ÙŠØ¨Ùˆ Ø±Ø§Ø³ÙŠ Ù„ÙƒÙ† Ø§Ù„ÙŠÙˆÙ…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ØªÙˆÙ‚Ø¹Øª Ø§Ø°Ø§ Ø¬Ø§Øª Ø¯Ø§Ø±ÙŠØ§ Ø¨Ø´ÙˆÙÙ‡Ù… ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨Ø³ Ù„ÙŠ Ù„Ù„Ø­ÙŠÙ† ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>#Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§ÙƒØªØ¨ ØªÙˆÙ‚Ø¹Ùƒ Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‡Ù„Ø§Ù„ Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Ù†Ø¹Ù…Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© . ØªØ¶Ø¹ Ù‚Ø·Ø±Ø©ğŸ’§Ù…Ø¶Ø§Ø¯ Ø¨Ù†Ø³Ù„ÙŠÙ† Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Ø§Ù„Ø¯ÙˆØ¯Ùˆ Ø¬Ø§ÙŠÙ‡ ØªÙƒÙ…Ù„ Ø¹Ù„ÙŠ ğŸ’”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  Ø§Ø¹ØªØ±Ù Ø§Ù† Ø¨ØªØ³ ÙƒØ§Ù†Ùˆ Ø´ÙˆÙŠ Ø´ÙˆÙŠ ÙŠØ¬ÙŠØ¨Ùˆ Ø±Ø§Ø³ÙŠ Ù„ÙƒÙ† Ø§Ù„ÙŠÙˆÙ…...\n",
       "1      0  ØªÙˆÙ‚Ø¹Øª Ø§Ø°Ø§ Ø¬Ø§Øª Ø¯Ø§Ø±ÙŠØ§ Ø¨Ø´ÙˆÙÙ‡Ù… ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨Ø³ Ù„ÙŠ Ù„Ù„Ø­ÙŠÙ† ...\n",
       "2      0  #Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§ÙƒØªØ¨ ØªÙˆÙ‚Ø¹Ùƒ Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‡Ù„Ø§Ù„ Ùˆ...\n",
       "3      0  Ù†Ø¹Ù…Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© . ØªØ¶Ø¹ Ù‚Ø·Ø±Ø©ğŸ’§Ù…Ø¶Ø§Ø¯ Ø¨Ù†Ø³Ù„ÙŠÙ† Ø¹...\n",
       "4      0                             Ø§Ù„Ø¯ÙˆØ¯Ùˆ Ø¬Ø§ÙŠÙ‡ ØªÙƒÙ…Ù„ Ø¹Ù„ÙŠ ğŸ’”"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.8.0-cp310-cp310-win_amd64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading pymongo-4.8.0-cp310-cp310-win_amd64.whl (582 kB)\n",
      "   ---------------------------------------- 0.0/582.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 582.0/582.0 kB 5.1 MB/s eta 0:00:00\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.6.1 pymongo-4.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into MongoDB collection 'total_tweets'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the DataFrame (assuming you've already done this)\n",
    "df = pd.read_csv(r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\Arabic-Sentiment-Analysis-using-Arabic-BERT\\twitter_data.csv')\n",
    "\n",
    "# Convert the DataFrame to a dictionary format suitable for MongoDB\n",
    "data_dict = df.to_dict(\"records\")\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Adjust the URI if necessary\n",
    "\n",
    "# Specify the database and collection\n",
    "db = client['twitter']  # Replace 'my_database' with your database name\n",
    "collection = db['total_tweets']\n",
    "\n",
    "# Insert the data into the collection\n",
    "collection.insert_many(data_dict)\n",
    "\n",
    "print(\"Data successfully inserted into MongoDB collection 'total_tweets'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"c:\\\\Users\\\\ahmed\\\\OneDrive\\\\Documents\\\\GitHub\\\\Arabic-Sentiment-Analysis-using-AraGPT2\\\\data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ahmed\\\\OneDrive\\\\Documents\\\\GitHub\\\\Arabic-Sentiment-Analysis-using-AraGPT2\\\\data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "  root_dir: Path\n",
    "  twitter_data: Path\n",
    "  Arabic_SS2030: Path\n",
    "  ar_reviews_100k: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constant import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir \n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import zipfile\n",
    "from Sentiment.logger import logger\n",
    "from Sentiment.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "    data_ingestion.extract_zip_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME :str = 'openai-community/roberta-base-openai-detector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from Sentiment.entity.artifact_entity import ModelTrainerArtifact, DataTransformationArtifact\n",
    "from Sentiment.entity.config_entity import ModelTrainerConfig\n",
    "from Sentiment.exception import SentimentException\n",
    "from Sentiment.utils.main_utils import save_object, load_object\n",
    "from Sentiment.constant.training_pipeline import MODEL_NAME\n",
    "import numpy as np\n",
    "from Sentiment.logger import logging\n",
    "from numpy import ndarray\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassifier(nn.Module):\n",
    "    def __init__(self, tokenizer=None, freeze_roberta=False):\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "\n",
    "        D_in = 768  # Input dimension based on the roberta-base model\n",
    "        H, D_out = 50, 2  # Intermediate layer size and output dimension\n",
    "        self.tokenizer = tokenizer\n",
    "        self.roberta = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        if freeze_roberta:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Debugging: Check input tensor shapes\n",
    "        print(f\"Forward pass input_ids shape: {input_ids.shape}\")\n",
    "        print(f\"Forward pass attention_mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        if input_ids.size(0) == 0:\n",
    "            raise ValueError(\"Empty input_ids tensor\")\n",
    "        if attention_mask.size(0) == 0:\n",
    "            raise ValueError(\"Empty attention_mask tensor\")\n",
    "\n",
    "        # Ensure the sequence length is within the model's maximum sequence length\n",
    "        if input_ids.size(1) > self.roberta.config.max_position_embeddings:\n",
    "            raise ValueError(f\"Sequence length {input_ids.size(1)} exceeds maximum allowed length \"\n",
    "                             f\"{self.roberta.config.max_position_embeddings}\")\n",
    "\n",
    "        # Feed input to RoBERTa\n",
    "        try:\n",
    "            outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during RoBERTa forward pass: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Extract the last hidden state of the token [CLS] for classification task\n",
    "        try:\n",
    "            last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "            print(f\"Last hidden state [CLS] shape: {last_hidden_state_cls.shape}\")\n",
    "        except IndexError as e:\n",
    "            print(f\"Index error in extracting [CLS] token: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        try:\n",
    "            logits = self.classifier(last_hidden_state_cls)\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during classifier forward pass: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def preprocess_data(texts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Preprocesses the input texts by tokenizing and converting to tensors.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of input strings to be tokenized.\n",
    "        tokenizer (AutoTokenizer): The tokenizer instance from Hugging Face.\n",
    "        max_length (int): The maximum sequence length for the tokens.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (torch.Tensor): Tensor containing token ids.\n",
    "        attention_mask (torch.Tensor): Tensor containing attention masks.\n",
    "    \"\"\"\n",
    "    # Tokenize the input texts\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    return input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass input_ids shape: torch.Size([2, 141])\n",
      "Forward pass attention_mask shape: torch.Size([2, 141])\n",
      "Last hidden state [CLS] shape: torch.Size([2, 768])\n",
      "Logits shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Example texts to classify\n",
    "texts = [\"Example text for classification\", \"Another example text\"]\n",
    "\n",
    "# Preprocess the texts\n",
    "input_ids, attention_mask = preprocess_data(texts, tokenizer, max_length=141)\n",
    "\n",
    "# Forward pass through the model\n",
    "logits = model(input_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels have been converted to 0, 1, and 2.\n",
      "Inserted 99999 records into reviews collection in MongoDB.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "\n",
    "class TSVtoMongoDB:\n",
    "    def __init__(self, tsv_file_path, mongo_uri, database_name, collection_name):\n",
    "        self.tsv_file_path = tsv_file_path\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.database_name = database_name\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def read_tsv(self):\n",
    "        try:\n",
    "            # Read the TSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(self.tsv_file_path, sep='\\t')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading the TSV file: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def convert_labels(self, df):\n",
    "        \"\"\"\n",
    "        Convert 'label' column from 'Positive'/'Negative'/'Mixed' to 1/0/2.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if 'label' not in df.columns:\n",
    "                raise ValueError(\"The DataFrame does not contain a 'label' column.\")\n",
    "            \n",
    "            # Map the labels to 1, 0, and 2\n",
    "            df['label'] = df['label'].map({'Positive': 1, 'Negative': 0, 'Mixed': 2})\n",
    "            \n",
    "            if df['label'].isnull().any():\n",
    "                print(df['label'].unique())\n",
    "                raise ValueError(\"There are values in the 'label' column that are not mapped to 1, 0, or 2.\")\n",
    "            \n",
    "            print(\"Labels have been converted to 0, 1, and 2.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting labels: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def save_to_mongo(self, df):\n",
    "        try:\n",
    "            # Establish connection to MongoDB\n",
    "            client = MongoClient(self.mongo_uri)\n",
    "            db = client[self.database_name]\n",
    "            collection = db[self.collection_name]\n",
    "\n",
    "            # Convert DataFrame to a list of dictionaries and insert into MongoDB\n",
    "            data = df.to_dict(orient='records')\n",
    "            collection.insert_many(data)\n",
    "            print(f\"Inserted {len(data)} records into {self.collection_name} collection in MongoDB.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to MongoDB: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def process(self):\n",
    "        try:\n",
    "            df = self.read_tsv()\n",
    "            self.convert_labels(df)  # Convert labels before saving to MongoDB\n",
    "            self.save_to_mongo(df)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during processing: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    tsv_file_path = r\"C:\\Users\\ahmed\\Downloads\\ar_reviews_100k.tsv\"\n",
    "    mongo_uri = \"mongodb://localhost:27017/\"  # Replace with your MongoDB URI\n",
    "    database_name = \"ar_reviews\"\n",
    "    collection_name = \"reviews\"\n",
    "\n",
    "    tsv_to_mongo = TSVtoMongoDB(tsv_file_path, mongo_uri, database_name, collection_name)\n",
    "    tsv_to_mongo.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()))\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\env-sent\\lib\\site-packages\\torch\\cuda\\__init__.py:878\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\env-sent\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\env-sent\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\env-sent\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\env-sent\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB and fetched a document.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Replace with your actual connection string and collection details\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client['restaurant']\n",
    "collection = db['reviews']\n",
    "\n",
    "# Fetch one document to verify the connection\n",
    "doc = collection.find_one()\n",
    "if doc:\n",
    "    print(\"Successfully connected to MongoDB and fetched a document.\")\n",
    "else:\n",
    "    print(\"Connection successful, but no documents found in the collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into DataFrame. Shape: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch all documents from the collection\n",
    "data = list(collection.find())\n",
    "\n",
    "# Convert to DataFrame\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Data successfully loaded into DataFrame. Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"No data found in the collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66c2d092cb5e6a0889a6529e</td>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66c2d092cb5e6a0889a6529f</td>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66c2d092cb5e6a0889a652a0</td>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66c2d092cb5e6a0889a652a1</td>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66c2d092cb5e6a0889a652a2</td>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  66c2d092cb5e6a0889a6529e   \n",
       "1  66c2d092cb5e6a0889a6529f   \n",
       "2  66c2d092cb5e6a0889a652a0   \n",
       "3  66c2d092cb5e6a0889a652a1   \n",
       "4  66c2d092cb5e6a0889a652a2   \n",
       "\n",
       "                                              Review  Liked  \n",
       "0                           Wow... Loved this place.      1  \n",
       "1                                 Crust is not good.      0  \n",
       "2          Not tasty and the texture was just nasty.      0  \n",
       "3  Stopped by during the late May bank holiday of...      1  \n",
       "4  The selection on the menu was great and so wer...      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "araSent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
